---
version: v1alpha
# These are the new definitions which are used throughout the configuration.
definitions:
  helmConfigs:
    - &defaultHelm
      name: defaultHelm
      kind: helm
      repos:
        - name: atlas
          url: http://atlas.cnct.io
        - name: stable
          url: https://kubernetes-charts.storage.googleapis.com
      charts:
        - name: kubedns
          repo: atlas
          chart: kubedns
          version: 0.1.0
          namespace: kube-system
          values:
            cluster_ip: 10.32.0.2
            dns_domain: cluster.local
        - name: heapster
          repo: atlas
          chart: heapster
          version: 0.1.0
          namespace: kube-system
        - name: central-logging
          repo: atlas
          chart: central-logging
          version: 0.2.0
  fabricConfigs:
    - &defaultCanalFabric
      name: defaultCanalFabric
      kind: fabric
      type: canal
      options:
        containers:
          kubePolicyController:
            version: v0.5.1
            location: calico/kube-policy-controller
          etcd:
            version: v3.0.9
            location: quay.io/coreos/etcd
          calicoCni:
            version: v1.4.2
            location: calico/cni
          calicoNode:
            version: v1.0.0-rc1
            location: quay.io/calico/node
          flannel:
            version: v0.6.1
            location: quay.io/coreos/flannel
        network:
          network: 10.128.0.0/10
          subnetLen: 22
          subnetMin: 10.128.0.0
          subnetMax: 10.191.255.255
          backend:
            type: vxlan
  kvStoreConfigs:
    - &defaultEtcd
      name: etcd
      kind: kvStore
      type: etcd
      clientPorts: [2379, 4001]
      clusterToken: espouse-monger-rarely
      peerPorts: [2380]
      ssl: true
      version: v3.1.0
    - &defaultEtcdEvents
      name: etcdEvents
      kind: kvStore
      type: etcd
      clientPorts: [2381]
      clusterToken: animism-training-chastity
      peerPorts: [2382]
      ssl: true
      version: v3.1.0
  apiServerConfigs:
    - &defaultApiServer
      name: defaultApiServer
      kind: apiServer
      loadBalancer: cloud
      state:
        etcd: *defaultEtcd
      events:
        etcd: *defaultEtcdEvents
  kubeConfigs:
    - &defaultKube
      name: defaultKube
      kind: kubernetes
      version: v1.5.6
      hyperkubeLocation: gcr.io/google_containers/hyperkube
  containerConfigs:
    - &defaultDocker
      name: defaultDocker
      kind: container
      runtime: docker
      type: distro
  osConfigs:
    - &defaultCoreOs
      name: defaultCoreOs
      kind: os
      type: coreOs
      version: current
      channel: stable
      rebootStrategy: "off"
  nodeConfigs:
    - &defaultAwsEtcdNode
      name: defaultAwsEtcdNode
      kind: node
      mounts:
        -
          device: sdf
          path: /var/lib/docker
          forceFormat: true
        -
          device: sdg
          path: /ephemeral
          forceFormat: false
      providerConfig:
        provider: aws
        type: t2.small
        subnet: ["uwswest2a", "uwswest2b", "uwswest2c"]
        tags:
          -
            key: comments
            value: "Cluster etcd"
        storage:
          -
            type: ebs_block_device
            opts:
              device_name: sdf
              volume_type: gp2
              volume_size: 100
              delete_on_termination: true
              snapshot_id:
              encrypted: false
          -
            type: ebs_block_device
            opts:
              device_name: sdg
              volume_type: gp2
              volume_size: 10
              delete_on_termination: true
              snapshot_id:
              encrypted: false
    - &defaultAwsEtcdEventsNode
      name: defaultAwsEtcdEventsNode
      kind: node
      mounts:
        -
          device: sdf
          path: /var/lib/docker
          forceFormat: true
        -
          device: sdg
          path: /ephemeral
          forceFormat: false
      providerConfig:
        provider: aws
        type: t2.small
        subnet: ["uwswest2a", "uwswest2b", "uwswest2c"]
        tags:
          -
            key: comments
            value: "Cluster events etcd"
        storage:
          -
            type: ebs_block_device
            opts:
              device_name: sdf
              volume_type: gp2
              volume_size: 100
              delete_on_termination: true
              snapshot_id:
              encrypted: false
          -
            type: ebs_block_device
            opts:
              device_name: sdg
              volume_type: gp2
              volume_size: 10
              delete_on_termination: true
              snapshot_id:
              encrypted: false
    - &defaultAwsMasterNode
      name: defaultAwsMasterNode
      kind: node
      mounts:
        -
          device: sdf
          path: /var/lib/docker
          forceFormat: true
      providerConfig:
        provider: aws
        type: m3.medium
        subnet: ["uwswest2a", "uwswest2b", "uwswest2c"]
        tags:
          -
            key: comments
            value: "Master instances"
        storage:
          -
            type: ebs_block_device
            opts:
              device_name: sdf
              volume_type: gp2
              volume_size: 100
              delete_on_termination: true
              snapshot_id:
              encrypted: false
    - &defaultAwsClusterNode
      name: defaultAwsClusterNode
      kind: node
      mounts:
        -
          device: sdf
          path: /var/lib/docker
          forceFormat: true
      providerConfig:
        provider: aws
        type: c4.large
        subnet: ["uwswest2a", "uwswest2b", "uwswest2c"]
        tags:
          -
            key: comments
            value: "Cluster plain nodes"
        storage:
          -
            type: ebs_block_device
            opts:
              device_name: sdf
              volume_type: gp2
              volume_size: 100
              delete_on_termination: true
              snapshot_id:
              encrypted: false
    - &defaultAwsSpecialNode
      name: defaultAwsSpecialNode
      kind: node
      mounts:
        -
          device: sdf
          path: /var/lib/docker
          forceFormat: true
      keypair: krakenKey
      providerConfig:
        provider: aws
        type: m3.medium
        subnet: ["uwswest2a", "uwswest2b", "uwswest2c"]
        tags:
          -
            key: comments
            value: "Cluster special nodes"
        storage:
          -
            type: ebs_block_device
            opts:
              device_name: sdf
              volume_type: gp2
              volume_size: 100
              delete_on_termination: true
              snapshot_id:
              encrypted: false
  providerConfigs:
    - &defaultAws
      name: defaultAws
      kind: provider
      provider: aws
      type: cloudinit
      resourcePrefix:
      vpc: 10.0.0.0/16
      region: us-west-2
      subnet:
        -
          name: uwswest2a
          az: us-west-2a
          cidr: 10.0.0.0/18
        -
          name: uwswest2b
          az: us-west-2b
          cidr: 10.0.64.0/18
        -
          name: uwswest2c
          az: us-west-2c
          cidr: 10.0.128.0/17
      egressAcl:
        -
          protocol: "-1"
          rule_no: 100
          action: "allow"
          cidr_block: "0.0.0.0/0"
          from_port: 0
          to_port: 0
      ingressAcl:
        -
          protocol: "-1"
          rule_no: 100
          action: "allow"
          cidr_block: "0.0.0.0/0"
          from_port: 0
          to_port: 0
      authentication:
        accessKey:
        accessSecret:
        credentialsFile: "$HOME/.aws/credentials"
        credentialsProfile:
      ingressSecurity:
        -
          from_port: 22
          to_port: 22
          protocol: "TCP"
          cidr_blocks: ["0.0.0.0/0"]
        -
          from_port: 443
          to_port: 443
          protocol: "TCP"
          cidr_blocks: ["0.0.0.0/0"]
      egressSecurity:
        -
          from_port: 0
          to_port: 0
          protocol: "-1"
          cidr_blocks: ["0.0.0.0/0"]
  keyPairs:
   - &defaultKeyPair
      name: defaultKeyPair
      kind: keyPair
      publickeyFile: "$HOME/.ssh/id_rsa.pub"
      privatekeyFile: "$HOME/.ssh/id_rsa"
  kubeAuth:
   - &defaultKubeAuth
      authz: {}
      authn:
        basic:
          -
            password: "ChangeMe"
            user: "admin"
        default_basic_user: "admin"

# This is the core of the new configuration.

deployment:
  clusters:
    - name:
      network: 10.32.0.0/12
      dns: 10.32.0.2
      domain: cluster.local
      providerConfig: *defaultAws
      nodePools:
        - name: etcd
          count: 5
          etcdConfig: *defaultEtcd
          containerConfig: *defaultDocker
          osConfig: *defaultCoreOs
          nodeConfig: *defaultAwsEtcdNode
          keyPair: *defaultKeyPair
        - name: etcdEvents
          count: 5
          etcdConfig: *defaultEtcdEvents
          containerConfig: *defaultDocker
          osConfig: *defaultCoreOs
          nodeConfig: *defaultAwsEtcdEventsNode
          keyPair: *defaultKeyPair
        - name: master
          count: 3
          apiServerConfig: *defaultApiServer
          kubeConfig: *defaultKube
          containerConfig: *defaultDocker
          osConfig: *defaultCoreOs
          nodeConfig: *defaultAwsMasterNode
          keyPair: *defaultKeyPair
        - name: clusterNodes
          count: 3
          kubeConfig: *defaultKube
          containerConfig: *defaultDocker
          osConfig: *defaultCoreOs
          nodeConfig: *defaultAwsClusterNode
          keyPair: *defaultKeyPair
        - name: specialNodes
          count: 2
          kubeConfig: *defaultKube
          containerConfig: *defaultDocker
          osConfig: *defaultCoreOs
          nodeConfig: *defaultAwsSpecialNode
          keyPair: *defaultKeyPair
      fabricConfig: *defaultCanalFabric
      helmConfig: *defaultHelm
      kubeAuth: *defaultKubeAuth
  readiness:
    type: exact
    value: 0
    wait: 600

# These are the old definitions included for backwards compatibility while we
# convert each Ansible task to reference the new definitions and organization.

  cluster:
  serviceCidr: 10.32.0.0/12
  serviceDNS: 10.32.0.2
  clusterDomain: cluster.local
  coreos:
    - *defaultCoreOs
  keypair:
    - *defaultKeyPair
  kubeConfig:
    - *defaultKube
  containerConfig:
    - *defaultDocker
  fabric:
    provider: flannel
    <<: *defaultCanalFabric
  provider: aws
  providerConfig: *defaultAws
  kubeAuth: *defaultKubeAuth
  nodepool:
    -
      name: etcd
      count: 5
      <<: *defaultAwsEtcdNode
      keypair: *defaultKeyPair
      containerConfig: *defaultDocker
      coreos: *defaultCoreOs
    -
      name: etcdEvents
      count: 5
      <<: *defaultAwsEtcdEventsNode
      keypair: *defaultKeyPair
      containerConfig: *defaultDocker
      coreos: *defaultCoreOs
    -
      name: masterNodes
      count: 3
      <<: *defaultAwsMasterNode
      keypair: *defaultKeyPair
      kubeConfig: *defaultKube
      containerConfig: *defaultDocker
      coreos: *defaultCoreOs
    -
      name: clusterNodes
      count: 3
      <<: *defaultAwsClusterNode
      keypair: *defaultKeyPair
      kubeConfig: *defaultKube
      containerConfig: *defaultDocker
      coreos: *defaultCoreOs
    -
      name: specialNodes
      count: 2
      <<: *defaultAwsSpecialNode
      keypair: *defaultKeyPair
      kubeConfig: *defaultKube
      containerConfig: *defaultDocker
      coreos: *defaultCoreOs
  etcd:
    -
      name: etcd
      clientPorts: [2379, 4001]
      clusterToken: espouse-monger-rarely
      nodepool: etcd
      peerPorts: [2380]
      ssl: true
      version: v3.1.0
    -
      name: etcdEvents
      clientPorts: [2381]
      clusterToken: animism-training-chastity
      nodepool: etcdEvents
      peerPorts: [2382]
      ssl: true
      version: v3.1.0
  master:
    nodepool: masterNodes
    loadbalancer: cloud
    infra:
      etcd: etcd
    events:
      etcd: etcdEvents
  node:
    -
      name: clusterNodes
      nodepool: clusterNodes
    -
      name: specialNodes
      nodepool: specialNodes
  clusterServices:
    repos:
      -
        name: atlas
        url: http://atlas.cnct.io
    services:
      -
        name: kubedns
        repo: atlas
        chart: kubedns
        version: 0.1.0
        namespace: kube-system
        values:
          cluster_ip: 10.32.0.2
          dns_domain: cluster.local
      -
        name: heapster
        repo: atlas
        chart: heapster
        version: 0.1.0
        namespace: kube-system
